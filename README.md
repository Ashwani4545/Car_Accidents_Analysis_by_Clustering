# Car Accidents Analysis Using Clustering 

## Project Overview

This project analyzes vehicle accident records and groups them into **clusters** based on time, location, and severity patterns. The goal is to:
```
- Identify hidden behavioral and environmental patterns
- Support traffic safety planning
- Detect high-risk accident zones
- Enable future real-time risk scoring

Clustering algorithms used:

- **K-Means**
- **Gaussian Mixture Models (GMM)**

The end-to-end workflow includes:

- ETL preprocessing
- Feature engineering
- Scalable model training
- Experiment tracking
- Model serving via API
- Docker & Kubernetes deployment
- Full pipeline orchestration
```
---

## Key Features
```
âœ” Modern end-to-end MLOps workflow
âœ” Modular Python source code (`src/`)
âœ” Data versioning using **DVC**
âœ” Training experiments tracked using **MLflow**

âœ” Workflow orchestration using **Apache Airflow**

âœ” Real-time model inference using **FastAPI + Uvicorn**

âœ” Containerization using **Docker**

âœ” Production deployment via **Kubernetes manifests**

âœ” Monitoring using **Prometheus metrics exporter**

âœ” Optional distributed processing using **PySpark / Dask** stubs
```
---

# Architecture Diagram

```
Raw Data â†’ DVC Storage â†’ Preprocessing â†’ MLflow Tracking â†’ Model Registry
         â†“ Airflow DAG â†’ Training â†’ Best Model â†’ FastAPI Serving â†’ Monitoring (Prometheus)
                        â†“ Docker/K8s Deployment â†’ Autoscaling
```

---

# Technology Stack (Advanced)

### ğŸ§  Machine Learning

- Scikit-learn (Clustering Models)
- PySpark / Dask (Optional distributed pipeline)

### ğŸ“¦ Data Engineering

- **DVC** for dataset versioning
- **Airflow** for ETL orchestration
- **PyArrow** for optimized file handling

### ğŸ“Š Experiment Tracking & Model Registry

- **MLflow** (UI, metrics, parameters, artifact storage)

### ğŸš€ Model Serving & Deployment

- **FastAPI** for live model inference
- **Docker** for containerization
- **Kubernetes** deployment manifests

### ğŸ›  DevOps & Automation

- **GitHub Actions** (CI/CD pipeline)
- **Docker Compose** for local orchestrated environments

### ğŸ“¡ Monitoring & Observability

- **Prometheus client** for metrics
- Grafana (recommended setup)

---

## Project Structure

```
Car_Accidents_AdvancedStack_Project/
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data_preprocessing.py        # ETL processing
â”‚   â”œâ”€â”€ model_training_mlflow.py     # Training + MLflow logging
â”‚   â”œâ”€â”€ serve_fastapi.py             # API for model prediction
â”‚   â”œâ”€â”€ metrics_exporter.py          # Prometheus exporter
â”‚   â””â”€â”€ utils.py                     # Helper functions
â”‚
â”œâ”€â”€ mlflow/                          # MLflow configuration
â”œâ”€â”€ airflow/
â”‚   â””â”€â”€ dags/car_accidents_pipeline.py
â”‚
â”œâ”€â”€ deploy/
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”œâ”€â”€ k8s/deployment.yaml
â”‚   â””â”€â”€ k8s/service.yaml (optional)
â”‚
â”œâ”€â”€ dvc.yaml                         # DVC pipeline definition
â”œâ”€â”€ docker-compose.yml               # MLflow + API local stack
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ Makefile
â””â”€â”€ README.md
```

---

# How to Run the Project

### ğŸ”§ Install Dependencies

```
python -m venv venv
source venv/bin/activate
pip install -r requirements.txt
```

### ğŸ“‚ DVC Setup

```
dvc init
dvc repro          # runs `dvc.yaml` pipeline
```

### ğŸ”¬ MLflow Tracking

Start MLflow UI:

```
mlflow server \
    --backend-store-uri sqlite:///mlflow.db \
    --default-artifact-root ./mlruns \
    --host 0.0.0.0 --port 5000
```

Visit:

```
http://localhost:5000
```

### ğŸš¦ Run the Pipeline Manually

Preprocess Data

```
python src/data_preprocessing.py --input data/raw/accidents.csv --output data/processed/processed.csv
```

### Train Model

```
python src/model_training_mlflow.py --input data/processed/processed.csv --k 5 --output models/best_model.joblib
```

### ğŸŒ Run FastAPI Model Server

```
uvicorn src.serve_fastapi:app --host 0.0.0.0 --port 8080
```

### API Endpoints

```
GET  /health
POST /predict
```

**Example Request:**

```json
{
  "hour": 14,
  "dayofweek": 2,
  "month": 8,
  "severity": 3
}
```

### ğŸ³ Docker Deployment

Build the image:

```
docker build -t accident-api:latest ./deploy
```

Run the container:

```
docker run -p 8080:8080 accident-api:latest
```

### â˜¸ï¸ Kubernetes Deployment

Apply manifests:

```
kubectl apply -f deploy/k8s/deployment.yaml
```

Check pods:

```
kubectl get pods
```

### ğŸ“ˆ Monitoring with Prometheus

Start exporter:

```
python src/metrics_exporter.py
```

Visit metrics at:

```
http://localhost:8000
```

---

# CI/CD Pipeline (GitHub Actions)

Every push to `main` triggers:

- Dependency install
- Linting
- Future: automated DVC + MLflow jobs

YAML in:

```
.github/workflows/ci.yml
```

---

# Recommended Improvements

- Add DB integration (Snowflake, PostgreSQL, BigQuery)
- Create a Streamlit dashboard for interactive cluster visualization
- Add HDBSCAN + UMAP for advanced clustering
- Enable GPU-powered clustering
- Build full production observability via Grafana dashboards

---

# License

MIT License â€” free for personal and commercial use.
